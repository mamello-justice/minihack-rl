{"cells":[{"cell_type":"markdown","metadata":{"id":"-rQRFzP9-Q8g"},"source":["# Reinforcement Learning\n","\n","## Minihack"]},{"cell_type":"markdown","metadata":{"id":"VQSp-_GA-XrA"},"source":["### Install NLE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36382,"status":"ok","timestamp":1667651328047,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"},"user_tz":-120},"id":"6QwwVKfVNn67","outputId":"5c3d1695-1c51-4fef-e83a-bd5a905a6121"},"outputs":[],"source":["# !apt update -qq && apt install -qq -y flex bison libbz2-dev libglib2.0-0 libsm6 libxext6 cmake \n","# %pip install -U --quiet git+https://github.com/facebookresearch/nle.git@main"]},{"cell_type":"markdown","metadata":{},"source":["### Install Minihack"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# %pip install -U --quiet git+https://github.com/facebookresearch/minihack.git@main"]},{"cell_type":"markdown","metadata":{"id":"SpjV6Y_9EHAj"},"source":["### Install RLlib"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3833,"status":"ok","timestamp":1667651347655,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"},"user_tz":-120},"id":"Ge0Wos81EJ3A"},"outputs":[],"source":["%pip install -U --quiet ray[rllib] ray[tune] ray[default]"]},{"cell_type":"markdown","metadata":{"id":"Q6EIeHVf_q0Q"},"source":["### Installs"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4501,"status":"ok","timestamp":1667651352148,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"},"user_tz":-120},"id":"D3TEnWrn_xa3"},"outputs":[],"source":["%pip install -U --quiet comet_ml hydra-core pipdeptree wandb opencv-python"]},{"cell_type":"markdown","metadata":{"id":"y1r3jVBe_tLI"},"source":["### Versions"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3779,"status":"ok","timestamp":1667651355920,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"},"user_tz":-120},"id":"tpHSitsf7_X_","outputId":"7211ebb2-6dc8-4a9b-bd29-1b98e1e29390"},"outputs":[{"name":"stdout","output_type":"stream","text":["Python 3.10.6\n","gym==0.23.0\n","  - minihack==0.1.3+2f022b0 [requires: gym>=0.15,<=0.23]\n","  - nle==0.8.1+68b9362 [requires: gym>=0.15,<=0.23]\n","    - minihack==0.1.3+2f022b0 [requires: nle>=0.8.1]\n","pip==22.3.1\n","ray==2.0.1\n","wandb==0.13.5\n"]}],"source":["!python --version\n","!pipdeptree -r --packages pip,gym,nle,minihack,ray,wandb"]},{"cell_type":"markdown","metadata":{"id":"T51plL43H8o6"},"source":["### Imports"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":1612,"status":"ok","timestamp":1667651357527,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"},"user_tz":-120},"id":"NjwodT2cNosJ"},"outputs":[],"source":["import random\n","import gym\n","import nle\n","import minihack\n","import ray\n","\n","import numpy as np\n","import cv2\n","from collections import OrderedDict"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-11-08 17:28:57,445\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n"]},{"data":{"text/html":["<div>\n","    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n","        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n","        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n","            <g id=\"layer-1\">\n","                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n","                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n","                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n","                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n","                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n","                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n","                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n","                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n","                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n","                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n","                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n","                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n","                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n","                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n","                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n","                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n","                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n","            </g>\n","        </svg>\n","        <table>\n","            <tr>\n","                <td style=\"text-align: left\"><b>Python version:</b></td>\n","                <td style=\"text-align: left\"><b>3.10.6</b></td>\n","            </tr>\n","            <tr>\n","                <td style=\"text-align: left\"><b>Ray version:</b></td>\n","                <td style=\"text-align: left\"><b> 2.0.1</b></td>\n","            </tr>\n","            <tr>\n","    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n","    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n","</tr>\n","\n","        </table>\n","    </div>\n","</div>\n"],"text/plain":["RayContext(dashboard_url='127.0.0.1:8265', python_version='3.10.6', ray_version='2.0.1', ray_commit='03b6bc7b5a305877501110ec04710a9c57011479', address_info={'node_ip_address': '172.21.111.144', 'raylet_ip_address': '172.21.111.144', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-11-08_17-28-53_633699_1414/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-11-08_17-28-53_633699_1414/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-11-08_17-28-53_633699_1414', 'metrics_export_port': 51887, 'gcs_address': '172.21.111.144:43912', 'address': '172.21.111.144:43912', 'dashboard_agent_listen_port': 52365, 'node_id': '1f4a4ff01484e5dec3b8c54a42e19b0ed1b02c1e673ed42cfe1ba52c'})"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["ray.init(num_gpus=1)"]},{"cell_type":"markdown","metadata":{"id":"xC3t1oqU_bGy"},"source":["### Custom"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1667651357528,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"},"user_tz":-120},"id":"VfO0XdIY_eIj"},"outputs":[],"source":["from gym.spaces import Box\n","from minihack.envs.skills_quest import MiniHackQuestHard\n","from ray.tune.registry import register_env\n","\n","\n","class dotdict(dict):\n","    \"\"\"dot.notation access to dictionary attributes\"\"\"\n","\n","    __getattr__ = dict.get\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__\n","\n","\n","class CustomEnv(MiniHackQuestHard):\n","    def __init__(self, config):\n","        # Hack to resolve error \"'CustomEnv' object has no attribute 'env'\"\n","        self.env = dotdict({\"_vardir\": \"/tmp/run\"})\n","\n","        config = dotdict(config)\n","\n","        self._obs_keys = config.obs_keys.split(\",\")\n","        super().__init__(observation_keys=self._obs_keys)\n","\n","        self.shape = dotdict(config.input_shape)\n","        self.observation_space[\"pixel\"] = Box(\n","            0, 255, (self.shape.height, self.shape.width, 3), np.uint8\n","        )\n","\n","        print(self.observation_space)\n","\n","    def _resize_frame(self, frame):\n","        return cv2.resize(\n","            frame,\n","            dsize=(self.shape.width, self.shape.height),\n","            interpolation=cv2.INTER_LINEAR,\n","        )\n","\n","    def _process_obs(self, obs):\n","        return OrderedDict(\n","            {\n","                key: self._resize_frame(obs[key]) if key == \"pixel\" else obs[key]\n","                for key in self._obs_keys\n","            }\n","        )\n","\n","    def reset(self):\n","        return self._process_obs(super().reset())\n","\n","    def step(self, action):\n","        obs, reward, done, info = super().step(action)\n","        return self._process_obs(obs), reward, done, info\n","\n","\n","register_env(\"MiniHack-D3QN-v0\", CustomEnv)\n"]},{"cell_type":"markdown","metadata":{"id":"Rng4xmQzAydx"},"source":["### Train"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"aB8f0s9zN-WG","outputId":"6016e1ed-3b73-45cb-a0c6-a8a5a95f2e5f"},"outputs":[{"data":{"text/html":["== Status ==<br>Current time: 2022-11-08 17:51:12 (running for 00:03:34.77)<br>Memory usage on this node: 4.4/9.6 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 1.0/1 GPUs, 0.0/5.27 GiB heap, 0.0/2.64 GiB objects<br>Result logdir: /home/develop/github/minihack-rl/notebooks/results/DQN<br>Number of trials: 24/24 (11 ERROR, 12 PENDING, 1 RUNNING)<br><table>\n","<thead>\n","<tr><th>Trial name                      </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  gamma</th><th>hiddens  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  target_network_up...</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00011</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 10000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00012</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 10000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00013</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 10000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00014</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 10000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00015</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 10000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00016</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 50000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00017</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 50000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00018</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 50000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00019</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 50000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00020</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 50000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00021</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 50000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00022</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 50000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00023</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 50000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00000</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                  5000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00001</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                  5000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00002</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                  5000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00003</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                  5000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00004</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                  5000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00005</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                  5000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00006</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                  5000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00007</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                  5000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00008</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 10000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00009</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 10000</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00010</td><td>ERROR   </td><td>     </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 10000</td></tr>\n","</tbody>\n","</table><br>Number of errored trials: 11<br><table>\n","<thead>\n","<tr><th>Trial name                      </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                  </th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00000</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00000_0_gamma=0.9900,hiddens=256,lr=0.0001,target_network_update_freq=5000_2022-11-08_17-47-37/error.txt  </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00001</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00001_1_gamma=0.9990,hiddens=256,lr=0.0001,target_network_update_freq=5000_2022-11-08_17-47-58/error.txt  </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00002</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00002_2_gamma=0.9900,hiddens=512,lr=0.0001,target_network_update_freq=5000_2022-11-08_17-48-15/error.txt  </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00003</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00003_3_gamma=0.9990,hiddens=512,lr=0.0001,target_network_update_freq=5000_2022-11-08_17-48-31/error.txt  </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00004</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00004_4_gamma=0.9900,hiddens=256,lr=0.0010,target_network_update_freq=5000_2022-11-08_17-48-49/error.txt  </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00005</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00005_5_gamma=0.9990,hiddens=256,lr=0.0010,target_network_update_freq=5000_2022-11-08_17-49-09/error.txt  </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00006</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00006_6_gamma=0.9900,hiddens=512,lr=0.0010,target_network_update_freq=5000_2022-11-08_17-49-26/error.txt  </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00007</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00007_7_gamma=0.9990,hiddens=512,lr=0.0010,target_network_update_freq=5000_2022-11-08_17-49-43/error.txt  </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00008</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00008_8_gamma=0.9900,hiddens=256,lr=0.0001,target_network_update_freq=10000_2022-11-08_17-50-03/error.txt </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00009</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00009_9_gamma=0.9990,hiddens=256,lr=0.0001,target_network_update_freq=10000_2022-11-08_17-50-24/error.txt </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_ab7af_00010</td><td style=\"text-align: right;\">           1</td><td>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00010_10_gamma=0.9900,hiddens=512,lr=0.0001,target_network_update_freq=10000_2022-11-08_17-50-40/error.txt</td></tr>\n","</tbody>\n","</table><br>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dd6f2ece571f492cb0af1af77e5bd8ea","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016670803333302803, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m 2022-11-08 17:47:45,745\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m 2022-11-08 17:47:45,745\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m 2022-11-08 17:47:45,746\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m 2022-11-08 17:47:45,748\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m 2022-11-08 17:47:45,817\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00000_0_gamma=0.9900,hiddens=256,lr=0.0001,target_network_update_freq=5000_2022-11-08_17-47-37/wandb/run-20221108_174741-ab7af_00000</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00000\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00000</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-11-08 17:47:51,386\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00000: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2041, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m 2022-11-08 17:47:51,360\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2041, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=2041)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"47b0d4d01b8d43e590983a8e9e67d452","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.002 MB of 0.011 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.173712…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00000</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00000\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00000</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_174741-ab7af_00000/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00000:\n","  trial_id: ab7af_00000\n","  \n"]},{"name":"stderr","output_type":"stream","text":["2022-11-08 17:47:57,903\tERROR ray_trial_executor.py:103 -- An exception occurred when trying to stop the Ray actor:Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 94, in _post_stop_cleanup\n","    ray.get(future, timeout=0)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2041, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abf22e94dae5428ca6b328ce0627935c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016671568333307126, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m 2022-11-08 17:48:04,956\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m 2022-11-08 17:48:04,957\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m 2022-11-08 17:48:04,957\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m 2022-11-08 17:48:04,960\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m 2022-11-08 17:48:05,013\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00001_1_gamma=0.9990,hiddens=256,lr=0.0001,target_network_update_freq=5000_2022-11-08_17-47-58/wandb/run-20221108_174801-ab7af_00001</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00001\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00001</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-11-08 17:48:09,032\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00001: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2227, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m 2022-11-08 17:48:09,020\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2227, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=2227)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7535dafa4d34e8cae0c18e8340ed24d","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.002 MB of 0.011 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.173697…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00001</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00001\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00001</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_174801-ab7af_00001/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00001:\n","  trial_id: ab7af_00001\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0fb81fd11f14467c943bc645a2b69843","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669480000079298, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m 2022-11-08 17:48:20,934\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m 2022-11-08 17:48:20,934\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m 2022-11-08 17:48:20,935\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m 2022-11-08 17:48:20,936\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m 2022-11-08 17:48:20,975\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","2022-11-08 17:48:24,563\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00002: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2412, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m 2022-11-08 17:48:24,548\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2412, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=2412)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00002_2_gamma=0.9900,hiddens=512,lr=0.0001,target_network_update_freq=5000_2022-11-08_17-48-15/wandb/run-20221108_174818-ab7af_00002</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00002\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00002</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e6726e83c34494ab21544154eebd58a","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00002</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00002\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00002</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_174818-ab7af_00002/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00002:\n","  trial_id: ab7af_00002\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7042d5b3da1e40c985ff208d614fd088","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669351666253836, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m 2022-11-08 17:48:36,897\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m 2022-11-08 17:48:36,897\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m 2022-11-08 17:48:36,898\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m 2022-11-08 17:48:36,899\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m 2022-11-08 17:48:36,940\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","wandb: ERROR Failed to sample metric: Not Supported\n","2022-11-08 17:48:40,672\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00003: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2594, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m 2022-11-08 17:48:40,661\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2594, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=2594)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00003_3_gamma=0.9990,hiddens=512,lr=0.0001,target_network_update_freq=5000_2022-11-08_17-48-31/wandb/run-20221108_174835-ab7af_00003</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00003\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00003</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3914583b56a64ec4af82a21cb214a924","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00003</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00003\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00003</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_174835-ab7af_00003/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00003:\n","  trial_id: ab7af_00003\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"44f149b4de2642b88ec519be27d4e753","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669869999653506, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m 2022-11-08 17:48:55,455\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m 2022-11-08 17:48:55,455\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m 2022-11-08 17:48:55,456\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m 2022-11-08 17:48:55,458\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m 2022-11-08 17:48:55,523\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","2022-11-08 17:48:59,233\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00004: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2777, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m 2022-11-08 17:48:59,219\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2777, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=2777)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00004_4_gamma=0.9900,hiddens=256,lr=0.0010,target_network_update_freq=5000_2022-11-08_17-48-49/wandb/run-20221108_174852-ab7af_00004</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00004\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00004</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f2331443734b4b3ab510698df7723b89","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00004</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00004\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00004</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_174852-ab7af_00004/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00004:\n","  trial_id: ab7af_00004\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a5a756fcee94cb0928113602f44addc","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669206666604926, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m 2022-11-08 17:49:14,966\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m 2022-11-08 17:49:14,966\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m 2022-11-08 17:49:14,967\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m 2022-11-08 17:49:14,968\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m 2022-11-08 17:49:15,013\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","2022-11-08 17:49:18,366\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00005: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2972, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00005_5_gamma=0.9990,hiddens=256,lr=0.0010,target_network_update_freq=5000_2022-11-08_17-49-09/wandb/run-20221108_174912-ab7af_00005</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00005\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00005</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m 2022-11-08 17:49:18,353\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=2972, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=2972)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"175f7f52b08e43be81c073482ab6ba6a","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00005</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00005\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00005</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_174912-ab7af_00005/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00005:\n","  trial_id: ab7af_00005\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f9d54fe5a57453e875c4e79c4bc3dc9","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666979833341126, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m 2022-11-08 17:49:32,056\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m 2022-11-08 17:49:32,056\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m 2022-11-08 17:49:32,056\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m 2022-11-08 17:49:32,058\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m 2022-11-08 17:49:32,099\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00006_6_gamma=0.9900,hiddens=512,lr=0.0010,target_network_update_freq=5000_2022-11-08_17-49-26/wandb/run-20221108_174929-ab7af_00006</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00006\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00006</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-11-08 17:49:36,209\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00006: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3155, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m 2022-11-08 17:49:36,187\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3155, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=3155)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a582e6c1f718488597e0bd42738dc81c","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00006</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00006\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00006</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_174929-ab7af_00006/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00006:\n","  trial_id: ab7af_00006\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bf1c5875e264a878f197dbf92c2ec9f","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016671781666809694, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m 2022-11-08 17:49:49,920\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m 2022-11-08 17:49:49,920\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m 2022-11-08 17:49:49,921\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m 2022-11-08 17:49:49,923\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m 2022-11-08 17:49:49,992\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00007_7_gamma=0.9990,hiddens=512,lr=0.0010,target_network_update_freq=5000_2022-11-08_17-49-43/wandb/run-20221108_174946-ab7af_00007</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00007\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00007</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["2022-11-08 17:49:54,746\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00007: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3334, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n"]},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m 2022-11-08 17:49:54,729\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3334, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=3334)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cddf5f7054944a60984778040230aa6e","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00007</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00007\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00007</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_174946-ab7af_00007/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00007:\n","  trial_id: ab7af_00007\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55ffd07d814644b59d0c64fbedcf72e9","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666930833356067, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m 2022-11-08 17:50:09,382\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m 2022-11-08 17:50:09,383\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m 2022-11-08 17:50:09,383\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m 2022-11-08 17:50:09,385\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m 2022-11-08 17:50:09,431\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","wandb: ERROR Failed to sample metric: Not Supported\n","2022-11-08 17:50:14,476\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00008: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3524, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m 2022-11-08 17:50:14,452\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3524, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=3524)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00008_8_gamma=0.9900,hiddens=256,lr=0.0001,target_network_update_freq=10000_2022-11-08_17-50-03/wandb/run-20221108_175007-ab7af_00008</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00008\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00008</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00008:\n","  trial_id: ab7af_00008\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00008</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00008\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00008</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_175007-ab7af_00008/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4c04a501ed4745309d933fc5d666cfa3","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667124666661645, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m 2022-11-08 17:50:30,719\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m 2022-11-08 17:50:30,719\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m 2022-11-08 17:50:30,720\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m 2022-11-08 17:50:30,722\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m 2022-11-08 17:50:30,767\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","2022-11-08 17:50:30,867\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00009: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3691, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 595, in __init__\n","    raise RuntimeError(\n","RuntimeError: Found 0 GPUs on your machine (GPU devices found: [])! If your machine\n","    does not have any GPUs, you should set the config keys `num_gpus` and\n","    `num_gpus_per_worker` to 0 (they may be set to 1 by default for your\n","    particular RL algorithm).\n","To change the config for the `rllib train|rollout` command, use\n","  `--config={'[key]': '[value]'}` on the command line.\n","To change the config for `tune.run()` in a script: Modify the python dict\n","  passed to `tune.run(config=[...])`.\n","To change the config for an RLlib Algorithm instance: Modify the python dict\n","  passed to the Algorithm's constructor, e.g. `PPO(config=[...])`.\n","\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m /home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   warnings.warn(\"Can't initialize NVML\")\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m 2022-11-08 17:50:30,852\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3691, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 595, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     raise RuntimeError(\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m RuntimeError: Found 0 GPUs on your machine (GPU devices found: [])! If your machine\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     does not have any GPUs, you should set the config keys `num_gpus` and\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     `num_gpus_per_worker` to 0 (they may be set to 1 by default for your\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m     particular RL algorithm).\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m To change the config for the `rllib train|rollout` command, use\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   `--config={'[key]': '[value]'}` on the command line.\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m To change the config for `tune.run()` in a script: Modify the python dict\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   passed to `tune.run(config=[...])`.\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m To change the config for an RLlib Algorithm instance: Modify the python dict\n","\u001b[2m\u001b[36m(DQN pid=3691)\u001b[0m   passed to the Algorithm's constructor, e.g. `PPO(config=[...])`.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00009_9_gamma=0.9990,hiddens=256,lr=0.0001,target_network_update_freq=10000_2022-11-08_17-50-24/wandb/run-20221108_175029-ab7af_00009</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00009\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00009</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"734eee01936141a79822373bf037e4ba","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00009:\n","  trial_id: ab7af_00009\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00009</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00009\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00009</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_175029-ab7af_00009/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a7f8d678d4f40b59af8dbe1eac18110","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01666995166694202, max=1.0)…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m 2022-11-08 17:50:46,599\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m 2022-11-08 17:50:46,599\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m 2022-11-08 17:50:46,600\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m 2022-11-08 17:50:46,602\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m 2022-11-08 17:50:46,661\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m 2022-11-08 17:50:50,979\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3870, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=3870)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","2022-11-08 17:50:51,011\tERROR trial_runner.py:987 -- Trial DQN_MiniHack-D3QN-v0_ab7af_00010: Error processing event.\n","ray.tune.error._TuneNoNextExecutorEventError: Traceback (most recent call last):\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/execution/ray_trial_executor.py\", line 996, in get_next_executor_event\n","    future_result = ray.get(ready_future)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/client_mode_hook.py\", line 105, in wrapper\n","    return func(*args, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/_private/worker.py\", line 2282, in get\n","    raise value\n","ray.exceptions.RayActorError: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=3870, ip=172.21.111.144, repr=DQN)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","    super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","    self.setup(copy.deepcopy(self.config))\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","    self.workers = WorkerSet(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","    self._local_worker = self._make_worker(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","    worker = cls(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","    self._build_policy_map(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","    self.policy_map.create_policy(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","    self[policy_id] = create_policy_for_framework(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","    return policy_class(observation_space, action_space, merged_config)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","    self._initialize_loss_from_dummy_batch(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","    actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","    return self._compute_action_helper(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","    return func(self, *a, **k)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","    dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","    q_vals = compute_q_values(\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","    model_out, state = model(input_dict, state_batches or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","    nn_out, _ = self.flatten[i](\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","    res = self.forward(restored, state or [], seq_lens)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","    self._features = self._hidden_layers(self._last_flat_in)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","    return self._model(x)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","    input = module(input)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","    return F.linear(input, self.weight, self.bias)\n","RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00010_10_gamma=0.9900,hiddens=512,lr=0.0001,target_network_update_freq=10000_2022-11-08_17-50-40/wandb/run-20221108_175043-ab7af_00010</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00010\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00010</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6e6e699eb72e4878a45d89a34eef60a9","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='0.000 MB of 0.000 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Synced <strong style=\"color:#cdcd00\">DQN_MiniHack-D3QN-v0_ab7af_00010</strong>: <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00010\" target=\"_blank\">https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00010</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Find logs at: <code>./wandb/run-20221108_175043-ab7af_00010/logs</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Result for DQN_MiniHack-D3QN-v0_ab7af_00010:\n","  trial_id: ab7af_00010\n","  \n"]},{"name":"stderr","output_type":"stream","text":["Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmamello-justice\u001b[0m (\u001b[33mcoms-4061a-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"98698ec5ea5b445fb67239ae27f6deb0","version_major":2,"version_minor":0},"text/plain":["VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669704999852306, max=1.0…"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["wandb: ERROR Failed to sample metric: Not Supported\n","2022-11-08 17:51:06,014\tWARNING tune.py:686 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m 2022-11-08 17:51:07,727\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m 2022-11-08 17:51:07,727\tWARNING deprecation.py:47 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m 2022-11-08 17:51:07,727\tINFO simple_q.py:293 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m 2022-11-08 17:51:07,729\tINFO algorithm.py:355 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m 2022-11-08 17:51:07,786\tWARNING env.py:142 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"]},{"data":{"text/html":["Tracking run with wandb version 0.13.5"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/home/develop/github/minihack-rl/notebooks/results/DQN/DQN_MiniHack-D3QN-v0_ab7af_00011_11_gamma=0.9990,hiddens=512,lr=0.0001,target_network_update_freq=10000_2022-11-08_17-51-00/wandb/run-20221108_175103-ab7af_00011</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/coms-4061a-team/minihack-rl/runs/ab7af_00011\" target=\"_blank\">DQN_MiniHack-D3QN-v0_ab7af_00011</a></strong> to <a href=\"https://wandb.ai/coms-4061a-team/minihack-rl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m 2022-11-08 17:51:12,448\tERROR worker.py:756 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::DQN.__init__()\u001b[39m (pid=4073, ip=172.21.111.144, repr=DQN)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 312, in __init__\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     super().__init__(config=config, logger_creator=logger_creator, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/tune/trainable/trainable.py\", line 159, in __init__\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     self.setup(copy.deepcopy(self.config))\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/algorithm.py\", line 422, in setup\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     self.workers = WorkerSet(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 171, in __init__\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     self._local_worker = self._make_worker(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/worker_set.py\", line 661, in _make_worker\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     worker = cls(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 613, in __init__\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     self._build_policy_map(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1803, in _build_policy_map\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     self.policy_map.create_policy(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_map.py\", line 123, in create_policy\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     self[policy_id] = create_policy_for_framework(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/policy.py\", line 80, in create_policy_for_framework\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return policy_class(observation_space, action_space, merged_config)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy_template.py\", line 330, in __init__\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     self._initialize_loss_from_dummy_batch(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/policy.py\", line 1050, in _initialize_loss_from_dummy_batch\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     actions, state_outs, extra_outs = self.compute_actions_from_input_dict(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 319, in compute_actions_from_input_dict\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return self._compute_action_helper(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/utils/threading.py\", line 24, in wrapper\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return func(self, *a, **k)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/policy/torch_policy.py\", line 952, in _compute_action_helper\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     dist_inputs, dist_class, state_out = self.action_distribution_fn(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 234, in get_distribution_inputs_and_class\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     q_vals = compute_q_values(\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/algorithms/dqn/dqn_torch_policy.py\", line 413, in compute_q_values\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     model_out, state = model(input_dict, state_batches or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/complex_input_net.py\", line 207, in forward\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     nn_out, _ = self.flatten[i](\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/modelv2.py\", line 259, in __call__\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     res = self.forward(restored, state or [], seq_lens)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/fcnet.py\", line 146, in forward\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     self._features = self._hidden_layers(self._last_flat_in)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/ray/rllib/models/torch/misc.py\", line 169, in forward\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return self._model(x)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 204, in forward\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     input = module(input)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return forward_call(*input, **kwargs)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m   File \"/home/develop/anaconda3/envs/minihack_rl/lib/python3.10/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m     return F.linear(input, self.weight, self.bias)\n","\u001b[2m\u001b[36m(DQN pid=4073)\u001b[0m RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat1 in method wrapper_addmm)\n","2022-11-08 17:51:12,731\tERROR tune.py:754 -- Trials did not complete: [DQN_MiniHack-D3QN-v0_ab7af_00000, DQN_MiniHack-D3QN-v0_ab7af_00001, DQN_MiniHack-D3QN-v0_ab7af_00002, DQN_MiniHack-D3QN-v0_ab7af_00003, DQN_MiniHack-D3QN-v0_ab7af_00004, DQN_MiniHack-D3QN-v0_ab7af_00005, DQN_MiniHack-D3QN-v0_ab7af_00006, DQN_MiniHack-D3QN-v0_ab7af_00007, DQN_MiniHack-D3QN-v0_ab7af_00008, DQN_MiniHack-D3QN-v0_ab7af_00009, DQN_MiniHack-D3QN-v0_ab7af_00010, DQN_MiniHack-D3QN-v0_ab7af_00011, DQN_MiniHack-D3QN-v0_ab7af_00012, DQN_MiniHack-D3QN-v0_ab7af_00013, DQN_MiniHack-D3QN-v0_ab7af_00014, DQN_MiniHack-D3QN-v0_ab7af_00015, DQN_MiniHack-D3QN-v0_ab7af_00016, DQN_MiniHack-D3QN-v0_ab7af_00017, DQN_MiniHack-D3QN-v0_ab7af_00018, DQN_MiniHack-D3QN-v0_ab7af_00019, DQN_MiniHack-D3QN-v0_ab7af_00020, DQN_MiniHack-D3QN-v0_ab7af_00021, DQN_MiniHack-D3QN-v0_ab7af_00022, DQN_MiniHack-D3QN-v0_ab7af_00023]\n","2022-11-08 17:51:12,732\tINFO tune.py:758 -- Total run time: 221.18 seconds (214.76 seconds for the tuning loop).\n","2022-11-08 17:51:12,733\tWARNING tune.py:764 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"]}],"source":["from hydra import initialize, compose\n","from omegaconf import OmegaConf\n","\n","import ray\n","from ray import tune\n","from ray.air.callbacks.wandb import WandbLoggerCallback\n","\n","with initialize(version_base=None, config_path=\".\"):\n","    cfg = compose(config_name=\"config.yaml\")\n","\n","dqn_cfg = OmegaConf.to_object(cfg.get(\"minihack-d3qn\", {}))\n","\n","wandb_cfg = dqn_cfg.get(\"logger_config\", {}).get(\"wandb\", {})\n","\n","callbacks = [\n","    WandbLoggerCallback(\n","        project=wandb_cfg[\"project\"],\n","        group=wandb_cfg[\"group\"],\n","        api_key_file=wandb_cfg[\"api_key_file\"],\n","    )\n","]\n","\n","analysis = tune.run(\n","    \"DQN\",\n","    callbacks=callbacks,\n","    config=dqn_cfg,\n","    stop={\"training_iteration\": 10},\n","    local_dir=\"./results\",\n","    log_to_file=True,\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ray.shutdown()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["VQSp-_GA-XrA","rLD0kgqyxBVn","SpjV6Y_9EHAj","Q6EIeHVf_q0Q","y1r3jVBe_tLI","T51plL43H8o6","xC3t1oqU_bGy"],"provenance":[{"file_id":"https://github.com/mamello-justice/minihack-rl/blob/duel_dqn/notebooks/DuelDQN.ipynb","timestamp":1667496871298}]},"gpuClass":"standard","interpreter":{"hash":"7f2e9c92d6325c2f07ae0efa9a675a2a8072e4d87bf0d0acc2244bf818644e39"},"kernelspec":{"display_name":"Python 3.8.10 ('minihack')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}
