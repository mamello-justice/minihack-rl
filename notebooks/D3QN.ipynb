{"cells":[{"cell_type":"markdown","metadata":{"id":"-rQRFzP9-Q8g"},"source":["# Reinforcement Learning\n","\n","## Minihack"]},{"cell_type":"markdown","metadata":{"id":"VQSp-_GA-XrA"},"source":["### Install NLE"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6QwwVKfVNn67","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667651328047,"user_tz":-120,"elapsed":36382,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"}},"outputId":"5c3d1695-1c51-4fef-e83a-bd5a905a6121"},"outputs":[{"output_type":"stream","name":"stdout","text":["6 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","bison is already the newest version (2:3.0.4.dfsg-1build1).\n","flex is already the newest version (2.6.4-6).\n","libsm6 is already the newest version (2:1.2.2-1).\n","libxext6 is already the newest version (2:1.3.3-1).\n","libglib2.0-cil is already the newest version (2.12.40-2).\n","libglib2.0-cil-dev is already the newest version (2.12.40-2).\n","cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n","libbz2-dev is already the newest version (1.0.6-8.1ubuntu0.2).\n","libglib2.0-0 is already the newest version (2.56.4-0ubuntu0.18.04.9).\n","libglib2.0-bin is already the newest version (2.56.4-0ubuntu0.18.04.9).\n","libglib2.0-data is already the newest version (2.56.4-0ubuntu0.18.04.9).\n","libglib2.0-dev is already the newest version (2.56.4-0ubuntu0.18.04.9).\n","libglib2.0-dev-bin is already the newest version (2.56.4-0ubuntu0.18.04.9).\n","libglib2.0-doc is already the newest version (2.56.4-0ubuntu0.18.04.9).\n","libglib2.0-tests is already the newest version (2.56.4-0ubuntu0.18.04.9).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!apt update -qq && apt install -qq -y flex bison libbz2-dev libglib2.0 libsm6 libxext6 cmake \n","!pip install -U --quiet git+https://github.com/facebookresearch/nle.git@main"]},{"cell_type":"markdown","source":["### Install Minihack"],"metadata":{"id":"rLD0kgqyxBVn"}},{"cell_type":"code","source":["!pip install -U --quiet git+https://github.com/facebookresearch/minihack.git@main"],"metadata":{"id":"rWDWsQuixEBN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667651343830,"user_tz":-120,"elapsed":15794,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"}},"outputId":"e385ccba-0610-4122-c704-857db75fac76"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["### Install RLlib"],"metadata":{"id":"SpjV6Y_9EHAj"}},{"cell_type":"code","source":["!pip install -U --quiet ray[rllib] ray[tune] ray[default]"],"metadata":{"id":"Ge0Wos81EJ3A","executionInfo":{"status":"ok","timestamp":1667651347655,"user_tz":-120,"elapsed":3833,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q6EIeHVf_q0Q"},"source":["### Installs"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"D3TEnWrn_xa3","executionInfo":{"status":"ok","timestamp":1667651352148,"user_tz":-120,"elapsed":4501,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"}}},"outputs":[],"source":["!pip install -U --quiet comet_ml hydra-core pipdeptree wandb"]},{"cell_type":"markdown","metadata":{"id":"y1r3jVBe_tLI"},"source":["### Versions"]},{"cell_type":"code","source":["!python --version\n","!pip --version\n","!pipdeptree -r --packages gym,nle,minihack,ray,wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tpHSitsf7_X_","executionInfo":{"status":"ok","timestamp":1667651355920,"user_tz":-120,"elapsed":3779,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"}},"outputId":"7211ebb2-6dc8-4a9b-bd29-1b98e1e29390"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Python 3.7.15\n","pip 21.1.3 from /usr/local/lib/python3.7/dist-packages/pip (python 3.7)\n","Warning!!! Possibly conflicting dependencies found:\n","* ipython==7.9.0\n"," - jedi [required: >=0.10, installed: ?]\n","------------------------------------------------------------------------\n","gym==0.23.0\n","  - dopamine-rl==1.0.5 [requires: gym>=0.10.5]\n","  - minihack==0.1.3+2f022b0 [requires: gym>=0.15,<=0.23]\n","  - nle==0.8.1+68b9362 [requires: gym>=0.15,<=0.23]\n","    - minihack==0.1.3+2f022b0 [requires: nle>=0.8.1]\n","ray==2.0.1\n","wandb==0.13.5\n"]}]},{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"T51plL43H8o6"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"NjwodT2cNosJ","executionInfo":{"status":"ok","timestamp":1667651357527,"user_tz":-120,"elapsed":1612,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"}}},"outputs":[],"source":["import random\n","import gym\n","import nle\n","import minihack\n","\n","import numpy as np\n","import cv2\n","from collections import OrderedDict\n","from tqdm.auto import trange\n"]},{"cell_type":"markdown","source":["### Custom"],"metadata":{"id":"xC3t1oqU_bGy"}},{"cell_type":"code","source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","from gym.spaces import Box\n","from minihack.envs.skills_quest import MiniHackQuestHard\n","from ray.tune.registry import register_env\n","\n","class dotdict(dict):\n","    \"\"\"dot.notation access to dictionary attributes\"\"\"\n","    __getattr__ = dict.get\n","    __setattr__ = dict.__setitem__\n","    __delattr__ = dict.__delitem__\n","\n","\n","class CustomEnv(MiniHackQuestHard):\n","    def __init__(self, config):\n","        # Hack to resolve error \"'CustomEnv' object has no attribute 'env'\"\n","        self.env = dotdict({'_vardir': '/tmp/run'})\n","\n","        config = dotdict(config)\n","\n","        self._obs_keys = config.obs_keys.split(\",\")\n","        super().__init__(observation_keys=self._obs_keys)\n","\n","        self.shape = dotdict(config.input_shape)\n","        self.observation_space['pixel'] = Box(0, 255, (self.shape.height, self.shape.width, 3), np.uint8)\n","\n","\n","    def _resize_frame(self, frame):\n","        return cv2.resize(\n","            frame,\n","            dsize=(self.shape.width,self.shape.height),\n","            interpolation=cv2.INTER_LINEAR\n","        )\n","\n","    def _process_obs(self, obs):\n","        return OrderedDict({\n","            key: self._resize_frame(obs[key]) if key == 'pixel' else obs[key] for key in self._obs_keys\n","        })\n","\n","    def reset(self):\n","        return self._process_obs(super().reset())\n","\n","    def step(self, action):\n","        obs, reward, done, info = super().step(action)\n","        return self._process_obs(obs), reward, done, info\n","\n","register_env('MiniHack-D3QN-v0', CustomEnv)"],"metadata":{"id":"VfO0XdIY_eIj","executionInfo":{"status":"ok","timestamp":1667651357528,"user_tz":-120,"elapsed":4,"user":{"displayName":"Mamello Seboholi","userId":"05593226727789766470"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rng4xmQzAydx"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aB8f0s9zN-WG","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6016e1ed-3b73-45cb-a0c6-a8a5a95f2e5f"},"outputs":[{"output_type":"stream","name":"stderr","text":["2022-11-05 12:30:24,831\tINFO worker.py:1515 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n","Instructions for updating:\n","experimental_relax_shapes is deprecated, use reduce_retracing instead\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["== Status ==<br>Current time: 2022-11-05 13:26:31 (running for 00:56:00.40)<br>Memory usage on this node: 4.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.24 GiB heap, 0.0/3.62 GiB objects (0.0/1.0 accelerator_type:T4)<br>Result logdir: /content/results/DQN<br>Number of trials: 18/24 (16 PENDING, 1 RUNNING, 1 TERMINATED)<br><table>\n","<thead>\n","<tr><th>Trial name                      </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  gamma</th><th>hiddens  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  target_network_up...</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  num_recreated_wor...</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th></tr>\n","</thead>\n","<tbody>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00001</td><td>RUNNING   </td><td>172.28.0.12:15583</td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                  5000</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        2324.34 </td><td style=\"text-align: right;\"> 2000</td><td style=\"text-align: right;\">-6      </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">               -2.54</td><td style=\"text-align: right;\">               -9.46</td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00002</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                  5000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00003</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                  5000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00004</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                  5000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00005</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                  5000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00006</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                  5000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00007</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                  5000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00008</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 10000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00009</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 10000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00010</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 10000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00011</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 10000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00012</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 10000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00013</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 10000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00014</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.99 </td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 10000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00015</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.999</td><td>[512]    </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">                 10000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00016</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 50000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00017</td><td>PENDING   </td><td>                 </td><td style=\"text-align: right;\">  0.999</td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                 50000</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                      </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td></tr>\n","<tr><td>DQN_MiniHack-D3QN-v0_a2cda_00000</td><td>TERMINATED</td><td>172.28.0.12:15318</td><td style=\"text-align: right;\">  0.99 </td><td>[256]    </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">                  5000</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         552.819</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">-6.98615</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">               -1.66</td><td style=\"text-align: right;\">               -9.45</td></tr>\n","</tbody>\n","</table><br><br>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=15318)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n","\u001b[2m\u001b[36m(pid=15318)\u001b[0m Instructions for updating:\n","\u001b[2m\u001b[36m(pid=15318)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n","\u001b[2m\u001b[36m(pid=15318)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n","\u001b[2m\u001b[36m(pid=15318)\u001b[0m Instructions for updating:\n","\u001b[2m\u001b[36m(pid=15318)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:42,418\tWARNING deprecation.py:48 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:42,418\tWARNING deprecation.py:48 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:42,419\tINFO simple_q.py:294 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:42,421\tINFO algorithm.py:358 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:42,543\tWARNING env.py:143 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:49,905\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:50,255\tWARNING deprecation.py:48 -- DeprecationWarning: `ReplayBuffer.add_batch()` has been deprecated. Use `ReplayBuffer.add()` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:50,263\tWARNING replay_buffer.py:61 -- Estimated max memory usage for replay buffer is 4.15669 GB (10000.0 batches of size 1, 415669 bytes each), available system memory is 13.616361472 GB\n","\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:30:50,264\tWARNING multi_agent_prioritized_replay_buffer.py:221 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n"]},{"output_type":"stream","name":"stdout","text":["Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 1000\n","  counters:\n","    num_agent_steps_sampled: 1000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 1000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-31-48\n","  done: false\n","  episode_len_mean: 1000.0\n","  episode_media: {}\n","  episode_reward_max: -9.11999999999985\n","  episode_reward_mean: -9.11999999999985\n","  episode_reward_min: -9.11999999999985\n","  episodes_this_iter: 1\n","  episodes_total: 1\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 1000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 1000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 1\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 1000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 1000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 99.99761904761904\n","    ram_util_percent: 30.75\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.1510228072251235\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 26.78335058343756\n","    mean_inference_ms: 11.119904456200539\n","    mean_raw_obs_processing_ms: 12.175107931161856\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 1000.0\n","    episode_media: {}\n","    episode_reward_max: -9.11999999999985\n","    episode_reward_mean: -9.11999999999985\n","    episode_reward_min: -9.11999999999985\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths:\n","      - 1000\n","      episode_reward:\n","      - -9.11999999999985\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.1510228072251235\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 26.78335058343756\n","      mean_inference_ms: 11.119904456200539\n","      mean_raw_obs_processing_ms: 12.175107931161856\n","  time_since_restore: 58.97095847129822\n","  time_this_iter_s: 58.97095847129822\n","  time_total_s: 58.97095847129822\n","  timers:\n","    training_iteration_time_ms: 200.261\n","  timestamp: 1667651508\n","  timesteps_since_restore: 0\n","  timesteps_total: 1000\n","  training_iteration: 1\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 2000\n","  counters:\n","    num_agent_steps_sampled: 2000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 2000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-32-40\n","  done: false\n","  episode_len_mean: 997.5\n","  episode_media: {}\n","  episode_reward_max: -9.11999999999985\n","  episode_reward_mean: -9.284999999999847\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 1\n","  episodes_total: 2\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 2000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 2000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 2\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 2000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 2000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 100.0\n","    ram_util_percent: 31.515068493150682\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.14721331892077677\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.96932709100987\n","    mean_inference_ms: 10.663820989501277\n","    mean_raw_obs_processing_ms: 11.883718969588628\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 997.5\n","    episode_media: {}\n","    episode_reward_max: -9.11999999999985\n","    episode_reward_mean: -9.284999999999847\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths:\n","      - 1000\n","      - 995\n","      episode_reward:\n","      - -9.11999999999985\n","      - -9.449999999999843\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.14721331892077677\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.96932709100987\n","      mean_inference_ms: 10.663820989501277\n","      mean_raw_obs_processing_ms: 11.883718969588628\n","  time_since_restore: 110.4480299949646\n","  time_this_iter_s: 51.47707152366638\n","  time_total_s: 110.4480299949646\n","  timers:\n","    training_iteration_time_ms: 216.923\n","  timestamp: 1667651560\n","  timesteps_since_restore: 0\n","  timesteps_total: 2000\n","  training_iteration: 2\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 3000\n","  counters:\n","    num_agent_steps_sampled: 3000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 3000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-33-32\n","  done: false\n","  episode_len_mean: 861.6666666666666\n","  episode_media: {}\n","  episode_reward_max: -5.469999999999928\n","  episode_reward_mean: -8.013333333333208\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 1\n","  episodes_total: 3\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 3000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 3000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 3\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 3000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 3000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 99.99054054054054\n","    ram_util_percent: 32.35945945945945\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.1457064940465431\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.601299221796012\n","    mean_inference_ms: 10.422329122574427\n","    mean_raw_obs_processing_ms: 11.784574022723868\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 861.6666666666666\n","    episode_media: {}\n","    episode_reward_max: -5.469999999999928\n","    episode_reward_mean: -8.013333333333208\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths:\n","      - 1000\n","      - 995\n","      - 590\n","      episode_reward:\n","      - -9.11999999999985\n","      - -9.449999999999843\n","      - -5.469999999999928\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.1457064940465431\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.601299221796012\n","      mean_inference_ms: 10.422329122574427\n","      mean_raw_obs_processing_ms: 11.784574022723868\n","  time_since_restore: 162.73212361335754\n","  time_this_iter_s: 52.284093618392944\n","  time_total_s: 162.73212361335754\n","  timers:\n","    training_iteration_time_ms: 220.854\n","  timestamp: 1667651612\n","  timesteps_since_restore: 0\n","  timesteps_total: 3000\n","  training_iteration: 3\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 4000\n","  counters:\n","    num_agent_steps_sampled: 4000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 4000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-34-25\n","  done: false\n","  episode_len_mean: 753.0\n","  episode_media: {}\n","  episode_reward_max: -1.6600000000000013\n","  episode_reward_mean: -7.007999999999894\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 2\n","  episodes_total: 5\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 4000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 4000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 4\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 4000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 4000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 100.0\n","    ram_util_percent: 32.97432432432432\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.14606797831211787\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.339048973285912\n","    mean_inference_ms: 10.166705307055334\n","    mean_raw_obs_processing_ms: 11.62111262564321\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 753.0\n","    episode_media: {}\n","    episode_reward_max: -1.6600000000000013\n","    episode_reward_mean: -7.007999999999894\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 2\n","    hist_stats:\n","      episode_lengths: [1000, 995, 590, 1000, 180]\n","      episode_reward: [-9.11999999999985, -9.449999999999843, -5.469999999999928, -9.339999999999845,\n","        -1.6600000000000013]\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.14606797831211787\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.339048973285912\n","      mean_inference_ms: 10.166705307055334\n","      mean_raw_obs_processing_ms: 11.62111262564321\n","  time_since_restore: 215.12863278388977\n","  time_this_iter_s: 52.39650917053223\n","  time_total_s: 215.12863278388977\n","  timers:\n","    training_iteration_time_ms: 222.047\n","  timestamp: 1667651665\n","  timesteps_since_restore: 0\n","  timesteps_total: 4000\n","  training_iteration: 4\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 5000\n","  counters:\n","    num_agent_steps_sampled: 5000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 5000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-35-19\n","  done: false\n","  episode_len_mean: 648.4285714285714\n","  episode_media: {}\n","  episode_reward_max: -1.6600000000000013\n","  episode_reward_mean: -6.029999999999915\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 2\n","  episodes_total: 7\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 5000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 5000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 5\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 5000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 5000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 100.0\n","    ram_util_percent: 33.145454545454534\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.145669283467572\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.24848584595616\n","    mean_inference_ms: 10.088812016463928\n","    mean_raw_obs_processing_ms: 11.534225023320271\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 648.4285714285714\n","    episode_media: {}\n","    episode_reward_max: -1.6600000000000013\n","    episode_reward_mean: -6.029999999999915\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 2\n","    hist_stats:\n","      episode_lengths: [1000, 995, 590, 1000, 180, 570, 204]\n","      episode_reward: [-9.11999999999985, -9.449999999999843, -5.469999999999928, -9.339999999999845,\n","        -1.6600000000000013, -5.34999999999993, -1.8200000000000014]\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.145669283467572\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.24848584595616\n","      mean_inference_ms: 10.088812016463928\n","      mean_raw_obs_processing_ms: 11.534225023320271\n","  time_since_restore: 269.6102726459503\n","  time_this_iter_s: 54.48163986206055\n","  time_total_s: 269.6102726459503\n","  timers:\n","    training_iteration_time_ms: 531.808\n","  timestamp: 1667651719\n","  timesteps_since_restore: 0\n","  timesteps_total: 5000\n","  training_iteration: 5\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 6000\n","  counters:\n","    num_agent_steps_sampled: 6000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 6000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-36-19\n","  done: false\n","  episode_len_mean: 692.375\n","  episode_media: {}\n","  episode_reward_max: -1.6600000000000013\n","  episode_reward_mean: -6.447499999999906\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 1\n","  episodes_total: 8\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 6000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 6000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 6\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 6000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 6000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 99.99176470588235\n","    ram_util_percent: 33.49176470588237\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.1455152930590488\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.296464754792826\n","    mean_inference_ms: 10.067704174642568\n","    mean_raw_obs_processing_ms: 11.525545391375246\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 692.375\n","    episode_media: {}\n","    episode_reward_max: -1.6600000000000013\n","    episode_reward_mean: -6.447499999999906\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths: [1000, 995, 590, 1000, 180, 570, 204, 1000]\n","      episode_reward: [-9.11999999999985, -9.449999999999843, -5.469999999999928, -9.339999999999845,\n","        -1.6600000000000013, -5.34999999999993, -1.8200000000000014, -9.369999999999845]\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.1455152930590488\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.296464754792826\n","      mean_inference_ms: 10.067704174642568\n","      mean_raw_obs_processing_ms: 11.525545391375246\n","  time_since_restore: 329.11572337150574\n","  time_this_iter_s: 59.50545072555542\n","  time_total_s: 329.11572337150574\n","  timers:\n","    training_iteration_time_ms: 218.354\n","  timestamp: 1667651779\n","  timesteps_since_restore: 0\n","  timesteps_total: 6000\n","  training_iteration: 6\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 7000\n","  counters:\n","    num_agent_steps_sampled: 7000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 7000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-37-12\n","  done: false\n","  episode_len_mean: 726.5555555555555\n","  episode_media: {}\n","  episode_reward_max: -1.6600000000000013\n","  episode_reward_mean: -6.726666666666566\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 1\n","  episodes_total: 9\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 7000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 7000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 7\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 7000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 7000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 100.0\n","    ram_util_percent: 34.20135135135135\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.14529824597209948\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.321349358785696\n","    mean_inference_ms: 10.040729489090005\n","    mean_raw_obs_processing_ms: 11.513349992526724\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 726.5555555555555\n","    episode_media: {}\n","    episode_reward_max: -1.6600000000000013\n","    episode_reward_mean: -6.726666666666566\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths: [1000, 995, 590, 1000, 180, 570, 204, 1000, 1000]\n","      episode_reward: [-9.11999999999985, -9.449999999999843, -5.469999999999928, -9.339999999999845,\n","        -1.6600000000000013, -5.34999999999993, -1.8200000000000014, -9.369999999999845,\n","        -8.959999999999853]\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.14529824597209948\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.321349358785696\n","      mean_inference_ms: 10.040729489090005\n","      mean_raw_obs_processing_ms: 11.513349992526724\n","  time_since_restore: 381.58123564720154\n","  time_this_iter_s: 52.4655122756958\n","  time_total_s: 381.58123564720154\n","  timers:\n","    training_iteration_time_ms: 196.29\n","  timestamp: 1667651832\n","  timesteps_since_restore: 0\n","  timesteps_total: 7000\n","  training_iteration: 7\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 8000\n","  counters:\n","    num_agent_steps_sampled: 8000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 8000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-38-04\n","  done: false\n","  episode_len_mean: 753.9\n","  episode_media: {}\n","  episode_reward_max: -1.6600000000000013\n","  episode_reward_mean: -6.993999999999895\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 1\n","  episodes_total: 10\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 8000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 8000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 8\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 8000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 8000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 99.99066666666667\n","    ram_util_percent: 34.88399999999999\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.14501942782752839\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.33624325614074\n","    mean_inference_ms: 10.011277458919313\n","    mean_raw_obs_processing_ms: 11.501945425868675\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 753.9\n","    episode_media: {}\n","    episode_reward_max: -1.6600000000000013\n","    episode_reward_mean: -6.993999999999895\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths: [1000, 995, 590, 1000, 180, 570, 204, 1000, 1000, 1000]\n","      episode_reward: [-9.11999999999985, -9.449999999999843, -5.469999999999928, -9.339999999999845,\n","        -1.6600000000000013, -5.34999999999993, -1.8200000000000014, -9.369999999999845,\n","        -8.959999999999853, -9.399999999999844]\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.14501942782752839\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.33624325614074\n","      mean_inference_ms: 10.011277458919313\n","      mean_raw_obs_processing_ms: 11.501945425868675\n","  time_since_restore: 434.23744082450867\n","  time_this_iter_s: 52.65620517730713\n","  time_total_s: 434.23744082450867\n","  timers:\n","    training_iteration_time_ms: 214.852\n","  timestamp: 1667651884\n","  timesteps_since_restore: 0\n","  timesteps_total: 8000\n","  training_iteration: 8\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 9000\n","  counters:\n","    num_agent_steps_sampled: 9000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 9000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_12-38-57\n","  done: false\n","  episode_len_mean: 733.6666666666666\n","  episode_media: {}\n","  episode_reward_max: -1.6600000000000013\n","  episode_reward_mean: -6.789166666666564\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 2\n","  episodes_total: 12\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 9000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 9000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 9\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 9000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 9000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 100.0\n","    ram_util_percent: 35.19599999999999\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.14446749560888153\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.356116239727623\n","    mean_inference_ms: 9.956576322429983\n","    mean_raw_obs_processing_ms: 11.480069296063078\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 733.6666666666666\n","    episode_media: {}\n","    episode_reward_max: -1.6600000000000013\n","    episode_reward_mean: -6.789166666666564\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 2\n","    hist_stats:\n","      episode_lengths: [1000, 995, 590, 1000, 180, 570, 204, 1000, 1000, 1000, 1000,\n","        265]\n","      episode_reward: [-9.11999999999985, -9.449999999999843, -5.469999999999928, -9.339999999999845,\n","        -1.6600000000000013, -5.34999999999993, -1.8200000000000014, -9.369999999999845,\n","        -8.959999999999853, -9.399999999999844, -9.269999999999847, -2.259999999999996]\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.14446749560888153\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.356116239727623\n","      mean_inference_ms: 9.956576322429983\n","      mean_raw_obs_processing_ms: 11.480069296063078\n","  time_since_restore: 487.0576591491699\n","  time_this_iter_s: 52.820218324661255\n","  time_total_s: 487.0576591491699\n","  timers:\n","    training_iteration_time_ms: 216.592\n","  timestamp: 1667651937\n","  timesteps_since_restore: 0\n","  timesteps_total: 9000\n","  training_iteration: 9\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n"]},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(DQN pid=15318)\u001b[0m 2022-11-05 12:40:02,167\tWARNING deprecation.py:48 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"]},{"output_type":"stream","name":"stdout","text":["Result for DQN_MiniHack-D3QN-v0_a2cda_00000:\n","  agent_timesteps_total: 10000\n","  counters:\n","    last_target_update_ts: 10000\n","    num_agent_steps_sampled: 10000\n","    num_agent_steps_trained: 32\n","    num_env_steps_sampled: 10000\n","    num_env_steps_trained: 32\n","    num_target_updates: 1\n","  custom_metrics: {}\n","  date: 2022-11-05_12-40-03\n","  done: true\n","  episode_len_mean: 754.1538461538462\n","  episode_media: {}\n","  episode_reward_max: -1.6600000000000013\n","  episode_reward_mean: -6.986153846153741\n","  episode_reward_min: -9.449999999999843\n","  episodes_this_iter: 1\n","  episodes_total: 13\n","  experiment_id: 69cc267f84b546d192e6aea4eb23746f\n","  hostname: 1c147db97f6e\n","  info:\n","    last_target_update_ts: 10000\n","    learner:\n","      default_policy:\n","        custom_metrics: {}\n","        learner_stats:\n","          cur_lr: 0.0001\n","          max_q: 0.03945760428905487\n","          mean_q: 0.002244360279291868\n","          min_q: -0.029430756345391273\n","        mean_td_error: -0.0280128326267004\n","        model: {}\n","        td_error: [-0.021293779835104942, -0.01893448829650879, -0.04254109412431717,\n","          -0.023181157186627388, -0.021192995831370354, -0.022742334753274918, -0.00044289231300354004,\n","          -0.047746654599905014, -0.013754290528595448, -0.04133155196905136, -0.04433712363243103,\n","          -0.025979910045862198, -0.04272719472646713, -0.010869200341403484, -0.03796467185020447,\n","          -0.0030861590057611465, 0.004563411697745323, -0.023340173065662384, -0.032518722116947174,\n","          -0.011523958295583725, -0.008313619531691074, -0.03541355952620506, -0.03532414138317108,\n","          -0.009601699188351631, -0.02122204564511776, -0.02015192247927189, -0.07656824588775635,\n","          -0.07496209442615509, -0.018640771508216858, -0.08218418061733246, -0.02871449664235115,\n","          -0.004368934314697981]\n","    num_agent_steps_sampled: 10000\n","    num_agent_steps_trained: 32\n","    num_env_steps_sampled: 10000\n","    num_env_steps_trained: 32\n","    num_target_updates: 1\n","  iterations_since_restore: 10\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 10000\n","  num_agent_steps_trained: 32\n","  num_env_steps_sampled: 10000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 32\n","  num_env_steps_trained_this_iter: 32\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 32\n","  perf:\n","    cpu_util_percent: 100.0\n","    ram_util_percent: 35.37096774193548\n","  pid: 15318\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 0.144663984426981\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 25.401386053032336\n","    mean_inference_ms: 9.94891930370174\n","    mean_raw_obs_processing_ms: 11.489407748739454\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 754.1538461538462\n","    episode_media: {}\n","    episode_reward_max: -1.6600000000000013\n","    episode_reward_mean: -6.986153846153741\n","    episode_reward_min: -9.449999999999843\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths: [1000, 995, 590, 1000, 180, 570, 204, 1000, 1000, 1000, 1000,\n","        265, 1000]\n","      episode_reward: [-9.11999999999985, -9.449999999999843, -5.469999999999928, -9.339999999999845,\n","        -1.6600000000000013, -5.34999999999993, -1.8200000000000014, -9.369999999999845,\n","        -8.959999999999853, -9.399999999999844, -9.269999999999847, -2.259999999999996,\n","        -9.349999999999845]\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 0.144663984426981\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 25.401386053032336\n","      mean_inference_ms: 9.94891930370174\n","      mean_raw_obs_processing_ms: 11.489407748739454\n","  time_since_restore: 552.8189957141876\n","  time_this_iter_s: 65.7613365650177\n","  time_total_s: 552.8189957141876\n","  timers:\n","    learn_throughput: 90.046\n","    learn_time_ms: 355.372\n","    load_throughput: 560.951\n","    load_time_ms: 57.046\n","    synch_weights_time_ms: 0.041\n","    training_iteration_time_ms: 336.713\n","  timestamp: 1667652003\n","  timesteps_since_restore: 0\n","  timesteps_total: 10000\n","  training_iteration: 10\n","  trial_id: a2cda_00000\n","  warmup_time: 7.541135311126709\n","  \n"]},{"output_type":"stream","name":"stderr","text":["\u001b[2m\u001b[36m(pid=15583)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n","\u001b[2m\u001b[36m(pid=15583)\u001b[0m Instructions for updating:\n","\u001b[2m\u001b[36m(pid=15583)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n","\u001b[2m\u001b[36m(pid=15583)\u001b[0m WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:561: calling function (from tensorflow.python.eager.def_function) with experimental_relax_shapes is deprecated and will be removed in a future version.\n","\u001b[2m\u001b[36m(pid=15583)\u001b[0m Instructions for updating:\n","\u001b[2m\u001b[36m(pid=15583)\u001b[0m experimental_relax_shapes is deprecated, use reduce_retracing instead\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:43:47,702\tWARNING deprecation.py:48 -- DeprecationWarning: `ray.rllib.algorithms.dqn.dqn.DEFAULT_CONFIG` has been deprecated. Use `ray.rllib.algorithms.dqn.dqn.DQNConfig(...)` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:43:47,832\tWARNING deprecation.py:48 -- DeprecationWarning: `config['multiagent']['replay_mode']` has been deprecated. config['replay_buffer_config']['replay_mode'] This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:43:47,833\tINFO simple_q.py:294 -- In multi-agent mode, policies will be optimized sequentially by the multi-GPU optimizer. Consider setting `simple_optimizer=True` if this doesn't work for you.\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:43:48,020\tINFO algorithm.py:358 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:43:51,528\tWARNING env.py:143 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:46:30,503\tINFO trainable.py:166 -- Trainable.setup took 162.801 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:46:30,504\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:46:38,472\tWARNING deprecation.py:48 -- DeprecationWarning: `ReplayBuffer.add_batch()` has been deprecated. Use `ReplayBuffer.add()` instead. This will raise an error in the future!\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:46:38,728\tWARNING replay_buffer.py:61 -- Estimated max memory usage for replay buffer is 4.42677 GB (10000.0 batches of size 1, 442677 bytes each), available system memory is 13.616361472 GB\n","\u001b[2m\u001b[36m(DQN pid=15583)\u001b[0m 2022-11-05 12:46:38,775\tWARNING multi_agent_prioritized_replay_buffer.py:221 -- Adding batches with column `weights` to this buffer while providing weights as a call argument to the add method results in the column being overwritten.\n"]},{"output_type":"stream","name":"stdout","text":["Result for DQN_MiniHack-D3QN-v0_a2cda_00001:\n","  agent_timesteps_total: 1000\n","  counters:\n","    num_agent_steps_sampled: 1000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 1000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_13-05-54\n","  done: false\n","  episode_len_mean: 278.0\n","  episode_media: {}\n","  episode_reward_max: -2.53999999999999\n","  episode_reward_mean: -2.53999999999999\n","  episode_reward_min: -2.53999999999999\n","  episodes_this_iter: 1\n","  episodes_total: 1\n","  experiment_id: 2d42823f93d540eb9ecce81cc99f6391\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 1000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 1000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 1\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 1000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 1000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 100.0\n","    ram_util_percent: 33.88325062034739\n","  pid: 15583\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 4.554749249697446\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 531.2504158629762\n","    mean_inference_ms: 204.16878153394154\n","    mean_raw_obs_processing_ms: 219.01317290611917\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 278.0\n","    episode_media: {}\n","    episode_reward_max: -2.53999999999999\n","    episode_reward_mean: -2.53999999999999\n","    episode_reward_min: -2.53999999999999\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths:\n","      - 278\n","      episode_reward:\n","      - -2.53999999999999\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 4.554749249697446\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 531.2504158629762\n","      mean_inference_ms: 204.16878153394154\n","      mean_raw_obs_processing_ms: 219.01317290611917\n","  time_since_restore: 1162.5891797542572\n","  time_this_iter_s: 1162.5891797542572\n","  time_total_s: 1162.5891797542572\n","  timers:\n","    training_iteration_time_ms: 3124.359\n","  timestamp: 1667653554\n","  timesteps_since_restore: 0\n","  timesteps_total: 1000\n","  training_iteration: 1\n","  trial_id: a2cda_00001\n","  warmup_time: 163.74539804458618\n","  \n","Result for DQN_MiniHack-D3QN-v0_a2cda_00001:\n","  agent_timesteps_total: 2000\n","  counters:\n","    num_agent_steps_sampled: 2000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 2000\n","    num_env_steps_trained: 0\n","  custom_metrics: {}\n","  date: 2022-11-05_13-25-16\n","  done: false\n","  episode_len_mean: 639.0\n","  episode_media: {}\n","  episode_reward_max: -2.53999999999999\n","  episode_reward_mean: -5.9999999999999165\n","  episode_reward_min: -9.459999999999843\n","  episodes_this_iter: 1\n","  episodes_total: 2\n","  experiment_id: 2d42823f93d540eb9ecce81cc99f6391\n","  hostname: 1c147db97f6e\n","  info:\n","    learner: {}\n","    num_agent_steps_sampled: 2000\n","    num_agent_steps_trained: 0\n","    num_env_steps_sampled: 2000\n","    num_env_steps_trained: 0\n","  iterations_since_restore: 2\n","  node_ip: 172.28.0.12\n","  num_agent_steps_sampled: 2000\n","  num_agent_steps_trained: 0\n","  num_env_steps_sampled: 2000\n","  num_env_steps_sampled_this_iter: 1000\n","  num_env_steps_trained: 0\n","  num_env_steps_trained_this_iter: 0\n","  num_faulty_episodes: 0\n","  num_healthy_workers: 0\n","  num_recreated_workers: 0\n","  num_steps_trained_this_iter: 0\n","  perf:\n","    cpu_util_percent: 99.99956629491945\n","    ram_util_percent: 36.59634448574969\n","  pid: 15583\n","  policy_reward_max: {}\n","  policy_reward_mean: {}\n","  policy_reward_min: {}\n","  sampler_perf:\n","    mean_action_processing_ms: 4.624586055125254\n","    mean_env_render_ms: 0.0\n","    mean_env_wait_ms: 538.0951013793679\n","    mean_inference_ms: 204.99823369025097\n","    mean_raw_obs_processing_ms: 217.4548517265317\n","  sampler_results:\n","    custom_metrics: {}\n","    episode_len_mean: 639.0\n","    episode_media: {}\n","    episode_reward_max: -2.53999999999999\n","    episode_reward_mean: -5.9999999999999165\n","    episode_reward_min: -9.459999999999843\n","    episodes_this_iter: 1\n","    hist_stats:\n","      episode_lengths:\n","      - 278\n","      - 1000\n","      episode_reward:\n","      - -2.53999999999999\n","      - -9.459999999999843\n","    num_faulty_episodes: 0\n","    policy_reward_max: {}\n","    policy_reward_mean: {}\n","    policy_reward_min: {}\n","    sampler_perf:\n","      mean_action_processing_ms: 4.624586055125254\n","      mean_env_render_ms: 0.0\n","      mean_env_wait_ms: 538.0951013793679\n","      mean_inference_ms: 204.99823369025097\n","      mean_raw_obs_processing_ms: 217.4548517265317\n","  time_since_restore: 2324.335854291916\n","  time_this_iter_s: 1161.7466745376587\n","  time_total_s: 2324.335854291916\n","  timers:\n","    training_iteration_time_ms: 4611.03\n","  timestamp: 1667654716\n","  timesteps_since_restore: 0\n","  timesteps_total: 2000\n","  training_iteration: 2\n","  trial_id: a2cda_00001\n","  warmup_time: 163.74539804458618\n","  \n"]}],"source":["from hydra import initialize, compose\n","from omegaconf import OmegaConf\n","from ray import tune\n","from ray.tune.logger import DEFAULT_LOGGERS\n","from ray.air.callbacks.wandb import WandbLoggerCallback\n","\n","with initialize(version_base=None, config_path=\".\"):\n","  cfg = compose(config_name='config.yaml')\n","\n","dqn_cfg = OmegaConf.to_object(cfg.get('minihack-d3qn', {}))\n","\n","wandb_cfg = dqn_cfg.get('logger_config', {}).get('wandb', {})\n","\n","callbacks = [\n","    WandbLoggerCallback(\n","        project=wandb_cfg['project'],\n","        group=wandb_cfg['group'],\n","        api_key_file=wandb_cfg['api_key_file']\n","    )\n","]\n","\n","analysis = tune.run(\n","    'DQN',\n","    callbacks=callbacks,\n","    config=dqn_cfg,\n","    stop={\"training_iteration\": 10},\n","    local_dir=\"./results\",\n","    log_to_file=True\n","    )"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/mamello-justice/minihack-rl/blob/duel_dqn/notebooks/DuelDQN.ipynb","timestamp":1667496871298}],"collapsed_sections":["VQSp-_GA-XrA","rLD0kgqyxBVn","SpjV6Y_9EHAj","Q6EIeHVf_q0Q","y1r3jVBe_tLI","T51plL43H8o6","xC3t1oqU_bGy"]},"interpreter":{"hash":"7f2e9c92d6325c2f07ae0efa9a675a2a8072e4d87bf0d0acc2244bf818644e39"},"kernelspec":{"display_name":"Python 3.8.10 ('minihack')","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}