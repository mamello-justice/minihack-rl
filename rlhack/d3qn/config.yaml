# Copyright (c) Facebook, Inc. and its affiliates.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#
defaults:
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog
  - override hydra/launcher: submitit_slurm

# pip install hydra-core hydra_colorlog
# can set these on the commandline too, e.g. `hydra.launcher.partition=dev`
hydra:
  launcher:
    timeout_min: 4300
    cpus_per_task: 10
    gpus_per_node: 1
    tasks_per_node: 1
    mem_gb: 20
    nodes: 1
    partition: dev
    comment: null
    max_num_timeout: 5 # will requeue on timeout or preemption

name: null # can use this to have multiple runs with same params, eg name=1,2,3,4,5
seed: null
seedspath: ""

# WandB settings
wandb:
  active: true # enable wandb logging
  project: minihack-rl # specifies the project name to log to
  entity: coms-4061a-team # the team to log to
  group: dqn # defines a group name for the experiment
  save_checkpoints: true # save model checkpoints to Wandb as artifacts
  api_key_file: ~/.wandb_api_key
  tags: # what tags for this run, comma-separated
    - rllib
    - dqn
    - duel
    - d3qn
    - minihack
    - nle

# Framework
framework: torch # not sure if tf is support

# Reward Function
reward_config:
  penalty_mode: constant
  penalty_step: -0.001
  penalty_time: 0.0
  reward_win: 1
  reward_lose: 0

character: mon-hum-neu-mal
save_tty: true
observation_keys:
  - glyphs
  - chars
  - colors
  - specials
  - blstats
  - message
env: MiniHack-Quest-Hard-v0

# Training settings.
num_gpus: 1
num_cpus: 1
num_actors: 256 # should be at least batch_size
total_steps: 1e9
train_batch_size: 32 # 32 is standard, can use 128 with small model variants
unroll_length: 80 # 80 is standard

# Evaluation settings.
evaluation_interval: 100
evaluation_duration: 50
evaluation_duration_unit: episodes
evaluation_config:
  explore: false
  render_env: true

# Model settings.
model: baseline # random, baseline, rnd, ride
use_lstm: true
hidden_dim: 256 # use at least 128, 256 is stronger
embedding_dim: 64 # use at least 32, 64 is stronger
glyph_type: all_cat # full, group_id, color_char, all, all_cat* (all_cat best, full fastest)
equalize_input_dim: false # project inputs to same dim (*false unless doing dynamics)
equalize_factor: 2 # multiplies hdim by this when equalize is enabled (2 > 1)
layers: 5 # number of cnn layers for crop/glyph model
crop_model: cnn
crop_dim: 9 # size of crop
use_index_select: true # use index select instead of normal embedding lookup

msg:
  model: lt_cnn # character model? none, lt_cnn*, cnn, gru, lstm
  hidden_dim: 64 # recommend 256
  embedding_dim: 32 # recommend 64

# Experimental settings.
state_counter: none # none, coordinates

# Generic Loss settings.
lr: 0.0002
gamma: 0.999 # probably a bit better at 0.999, esp with intrinsic reward
reward_clipping: none # use none with normalize_reward, else use tim

# Optimizer settings.
decay: 0.99 # 0.99 vs 0.9 vs 0.5 seems to make no difference
momentum: 0 # keep at 0
grad_clip: 40

# Algorithm-specific settings. These can override settings above (i.e. learning rate)
algo: dqn # must match one of the keys below

impala:
  entropy_coeff: 0.001 # 0.001 is better than 0.0001
  vf_loss_coeff: 0.5
  epsilon: 0.000001 # do not use 0.01, 1e-6 seems same as 1e-8

dqn:
  double_q: True
  dueling: True
  noisy: False
  replay_buffer_config:
    type: MultiAgentPrioritizedReplayBuffer
    capacity: 100000
  n_step: 5
  target_network_update_freq: 50000
  prioritized_replay_beta_annealing_timesteps: 500000
  num_steps_sampled_before_learning_starts: 50000
  model:
    use_lstm: False
  exploration_config:
    epsilon_timesteps: 1000000

ppo:
  rollout_fragment_length: 128
  train_batch_size: 128
  sgd_minibatch_size: 32
  num_sgd_iter: 2
  entropy_coeff: 0.0001
  vf_loss_coeff: 0.5
  model:
    vf_share_layers: True

a2c:
  rollout_fragment_length: 128
  train_batch_size: 128
  entropy_coeff: 0.001
  vf_loss_coeff: 0.1
  sample_async: False
